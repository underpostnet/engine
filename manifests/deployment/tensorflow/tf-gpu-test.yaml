---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tf-gpu-test-script
  namespace: default
data:
  main_tf_gpu_test.py: |
    import os
    import tensorflow as tf

    print("--- Starting GPU and Library Check ---")

    # --- Check for GPU devices ---
    gpus = tf.config.list_physical_devices("GPU")
    if gpus:
        try:
            # Restrict TensorFlow to only use the first GPU if multiple are detected.
            # This must be set before TensorFlow has initialized GPUs.
            tf.config.set_visible_devices(gpus[0], "GPU")
            logical_gpus = tf.config.list_logical_devices("GPU")
            print(
                f"TensorFlow detected {len(gpus)} Physical GPUs, "
                f"{len(logical_gpus)} Logical GPUs. Using: {gpus[0].name}"
            )
        except RuntimeError as e:
            # Catch potential errors if visible devices are set after GPU initialization
            print(f"RuntimeError during GPU configuration: {e}")
    else:
        print("TensorFlow did not detect any GPU devices. Running on CPU.")

    # --- Environment Variable Check Accelerated Linear Algebra ---
    xla_flags_env = os.environ.get("XLA_FLAGS")
    print(f"XLA_FLAGS environment variable (inside script): {xla_flags_env}")

    tf_xla_flags_env = os.environ.get("TF_XLA_FLAGS")
    print(f"TF_XLA_FLAGS environment variable (inside script): {tf_xla_flags_env}")

    # --- Verify TensorFlow version and CUDA/cuDNN status (optional but helpful) ---
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Is TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}")
    # tf.test.is_gpu_available() is deprecated in newer TF versions,
    # but still useful for a quick check.
    # Note: tf.test.is_gpu_available() is a deprecated function and may not work in future versions.
    # The recommended approach is to use tf.config.list_physical_devices("GPU").
    print(f"Is GPU available: {tf.test.is_gpu_available(cuda_only=False)}")

    # --- Final confirmation ---
    print("TensorFlow is configured to attempt running on GPU if available.")
    print("--- GPU and Library Check Complete ---")
---
apiVersion: v1
kind: Pod
metadata:
  name: tf-gpu-test-pod
  namespace: default
spec:
  restartPolicy: Never
  # CRITICAL: This line tells CRI-O to use the special NVIDIA runtime
  # which is configured to inject the GPU devices into the container.
  runtimeClassName: nvidia
  containers:
    - name: tensorflow-gpu-tester
      image: tensorflow/tensorflow:latest-gpu
      imagePullPolicy: IfNotPresent
      command: ['python']
      args: ['/app/main_tf_gpu_test.py']
      resources:
        limits:
          nvidia.com/gpu: '1'
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: 'all'
      volumeMounts:
        - name: tf-script-volume
          mountPath: /app
  volumes:
    - name: tf-script-volume
      configMap:
        name: tf-gpu-test-script
        items:
          - key: main_tf_gpu_test.py
            path: main_tf_gpu_test.py
